# ğŸ•‰ï¸ Sanskrit Document Retrieval-Augmented Generation (RAG) System

An AI-powered **Sanskrit Document Retrieval-Augmented Generation (RAG) system** built as part of an **AI/ML Intern Assignment**.  
The system retrieves relevant Sanskrit documents using vector similarity search and optionally generates contextual answers using a lightweight language model â€” all running **entirely on CPU**.

---

## ğŸ“Œ Project Objectives

- Enable efficient retrieval of Sanskrit text documents
- Support both **Sanskrit (Devanagari)** and **English** queries
- Implement a **CPU-only** Retrieval-Augmented Generation pipeline
- Maintain a **modular, explainable, and reproducible architecture**

---

## ğŸ§  System Architecture (High Level)

User Query
â†“
Text Embedding (Sentence Transformers)
â†“
FAISS Vector Search
â†“
Top-K Relevant Document Chunks
â†“
(Optional) LLM-based Answer Generation
â†“
Final Response



---

## ğŸ“‚ Project Structure


immverse AI/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ rag_pipeline.py
â”‚   â”œâ”€â”€ vector_store.py
â”‚   â”œâ”€â”€ document_loader.py
â”‚   â”œâ”€â”€ llm_generator.py
â”‚   â”œâ”€â”€ logger.py
â”‚   â”œâ”€â”€ config.py
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ *.txt
â”‚
â”œâ”€â”€ report/
â”‚   â””â”€â”€ Sanskrit_RAG_System_Report.pdf
â”‚
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ Sanskrit_RAG_Architecture.png   
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ venv/   (optional / not submitted)



---

## ğŸ“Š Dataset Description

- **Format:** Plain text (`.txt`)
- **Language:** Sanskrit (Devanagari) and English
- **Content:** Classical Sanskrit stories, prose passages, and moral narratives
- **Storage:** Local filesystem (offline, no external API dependency)
- **Encoding:** UTF-8

---

## âš™ï¸ Technologies Used

- **Python 3.9+**
- **Streamlit** â€“ Web interface
- **Sentence Transformers** â€“ Text embeddings
- **FAISS** â€“ Vector similarity search
- **Hugging Face Transformers** â€“ Optional language model generation
- **CPU-only inference** (no GPU required)

---

## ğŸš€ How to Run the Project

### 1ï¸âƒ£ Create & Activate Virtual Environment
```bash
python -m venv venv
venv\Scripts\activate   # Windows


pip install -r requirements.txt

streamlit run src/app.py

ğŸ” Example Queries

à¤®à¥‚à¤°à¥à¤–à¤­à¥ƒà¤¤à¥à¤¯à¤¸à¥à¤¯ à¤•à¤¥à¤¾ à¤•à¤¿à¤®à¥?

à¤•à¤¾à¤²à¥€à¤¦à¤¾à¤¸à¤¸à¥à¤¯ à¤šà¤¾à¤¤à¥à¤°à¥à¤¯à¤‚ à¤µà¤°à¥à¤£à¤¯à¤¤à¥

à¤ªà¥à¤°à¤¯à¤¤à¥à¤¨à¤¸à¥à¤¯ à¤®à¤¹à¤¤à¥à¤¤à¥à¤µà¤‚ à¤•à¤¿à¤®à¥?

What is the story of the foolish servant?

Tell me about Kalidasa's cleverness

ğŸ“ˆ Performance Observations

Latency: Sub-second retrieval for small document collections

Accuracy: Relevant document chunks retrieved for factual and story-based queries

Resource Usage: CPU-only, suitable for low-resource environments

âš ï¸ Limitations

Optimized for small to medium document collections

LLM-based generation on CPU may increase response time

Sanskrit semantic understanding depends on embedding quality

ğŸ”® Future Enhancements

Sanskrit-specific embedding models

Improved summarization of retrieved content

Larger corpus support

Advanced evaluation metrics